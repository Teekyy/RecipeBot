{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import env libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# import chatbot libraries\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "#import pdf embedding libraries\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "# import pinecone\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def initialize_pinecone(index_name):\n",
    "    pinecone.init(\n",
    "        api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "        environment=os.environ.get('PINECONE_ENVIRONMENT')\n",
    "    )\n",
    "\n",
    "    # create index if it does not exist\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(\n",
    "            index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "def initialize_chatbot(index_name):\n",
    "    chatbot = ChatOpenAI(\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        model='gpt-3.5-turbo',\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    vectorstore = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={'score_threshold': 0.9}\n",
    "    )\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        k=10,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key='answer',\n",
    "    )\n",
    "\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=chatbot,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        rephrase_question=False,\n",
    "        response_if_no_docs_found=\"Sorry, I have no information about that.\"\n",
    "    )\n",
    "\n",
    "    return qa\n",
    "\n",
    "def embed_pdfs(file_path):\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "    all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    Pinecone.from_documents(documents=all_splits, embedding=embeddings())\n",
    "\n",
    "def chat(query, qa):\n",
    "    response = qa.invoke(query)\n",
    "\n",
    "    print(response[\"answer\"])\n",
    "    print('\\n')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"recipes\"\n",
    "\n",
    "initialize_pinecone(index_name)\n",
    "chatbot = initialize_chatbot(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the recipe for boiled eggs?\"\n",
    "\n",
    "chat(query, chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me the nutritional information?\"\n",
    "\n",
    "chat(query, chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the length of the space station\"\n",
    "\n",
    "chat(query, chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory()\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"im good how are you\"}, {\"output\": \"im good too\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        model='gpt-3.5-turbo',\n",
    "        temperature=0\n",
    "    )\n",
    "template = (\n",
    "    \"Combine the chat history and follow up question into \"\n",
    "    \"a standalone question. Chat History: {chat_history}\"\n",
    "    \"Follow up question: {question}\"\n",
    ")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"question\"])\n",
    "memory = ConversationBufferWindowMemory(\n",
    "        k=10,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "memory.save_context({\"input\": \"how tall is john\"}, {\"output\": \"6 ft\"})\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "question_generator.invoke(\"how fast is he\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Combine the chat history and follow up question into a standalone question using the original language.\\n\\nChat History: Human: How tall is John?\\nAI: John is 6ft\\n\\nFollow up question: How much does he weigh?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = (\n",
    "    \"Combine the chat history and follow up question into a standalone question using the original language.\"\n",
    "    \"\\n\\nChat History: {chat_history}\"\n",
    "    \"\\n\\nFollow up question: {question}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt_value = prompt.invoke({\"chat_history\": \"Human: How tall is John?\\nAI: John is 6ft\", \"question\": \"How much does he weigh?\"})\n",
    "llm = ChatOpenAI(\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        model='gpt-3.5-turbo',\n",
    "        temperature=0\n",
    "    )\n",
    "# llm.invoke(prompt_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
